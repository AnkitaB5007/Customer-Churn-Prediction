{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4224c230",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction - Exploratory Data Analysis and Preprocessing\n",
    "\n",
    "This notebook provides a comprehensive analysis of the Telco Customer Churn dataset and implements a preprocessing pipeline for machine learning models.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Import Required Libraries](#1-import-required-libraries)\n",
    "2. [Load and Explore the Dataset](#2-load-and-explore-the-dataset)\n",
    "3. [Exploratory Data Analysis](#3-exploratory-data-analysis)\n",
    "4. [Data Preprocessing and Feature Engineering](#4-data-preprocessing-and-feature-engineering)\n",
    "5. [Train Multiple ML Models](#5-train-multiple-ml-models)\n",
    "6. [Model Evaluation and Comparison](#6-model-evaluation-and-comparison)\n",
    "7. [Create Streamlit Web Application](#7-create-streamlit-web-application)\n",
    "8. [Test the Streamlit App](#8-test-the-streamlit-app)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716a5278",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import necessary libraries including pandas, numpy, matplotlib, seaborn, scikit-learn, xgboost, and streamlit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc5aef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine learning libraries\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, \n",
    "    roc_curve, precision_recall_curve, accuracy_score\n",
    ")\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "\n",
    "# System and file operations\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting parameters\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìä Pandas version: {pd.__version__}\")\n",
    "print(f\"ü§ñ Scikit-learn version: {sklearn.__version__}\")\n",
    "print(f\"üöÄ XGBoost version: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a857407",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Dataset\n",
    "\n",
    "Load the Telco Customer Churn dataset from Kaggle and perform initial data exploration including shape, info, and basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba9b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    data_path = 'data/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Dataset loaded successfully from {data_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Dataset not found....\")\n",
    "\n",
    "# Display information about the dataset\n",
    "print(f\"\\n Dataset Shape: {df.shape}\")\n",
    "print(f\"Number of customers: {df.shape[0]:,}\")\n",
    "print(f\"Number of features: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ac8e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c499e2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd222d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa00740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values Analysis:\")\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Values': missing_values,\n",
    "    'Percentage': missing_percentage\n",
    "})\n",
    "\n",
    "missing_df = missing_df[missing_df['Missing Values'] > 0].sort_values('Missing Values', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    display(missing_df)\n",
    "else:\n",
    "    print(\"No missing values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f954b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable distribution\n",
    "print(\" Target Variable (Churn) Distribution:\")\n",
    "churn_counts = df['Churn'].value_counts()\n",
    "churn_percentage = df['Churn'].value_counts(normalize=True) * 100\n",
    "\n",
    "churn_summary = pd.DataFrame({\n",
    "    'Count': churn_counts,\n",
    "    'Percentage': churn_percentage\n",
    "})\n",
    "\n",
    "display(churn_summary)\n",
    "\n",
    "# Visualize target distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Count plot\n",
    "sns.countplot(data=df, x='Churn', ax=ax1)\n",
    "ax1.set_title('Churn Distribution (Count)')\n",
    "ax1.set_ylabel('Count')\n",
    "\n",
    "# Pie chart\n",
    "ax2.pie(churn_counts.values, labels=churn_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "ax2.set_title('Churn Distribution (Percentage)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "churn_rate = (df['Churn'] == 'Yes').mean() * 100\n",
    "print(f\"\\nOverall Churn Rate: {churn_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c05fb1",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis\n",
    "\n",
    "Visualize data distributions, correlations, and churn patterns using matplotlib and seaborn. Analyze categorical and numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a325fdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate categorical and numerical features\n",
    "categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Remove customerID from categorical features\n",
    "if 'customerID' in categorical_features:\n",
    "    categorical_features.remove('customerID')\n",
    "\n",
    "# Remove target from categorical features\n",
    "if 'Churn' in categorical_features:\n",
    "    categorical_features.remove('Churn')\n",
    "\n",
    "print(f\"Categorical Features ({len(categorical_features)}): {categorical_features}\")\n",
    "print(f\"Numerical Features ({len(numerical_features)}): {numerical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c8193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze categorical features\n",
    "print(\"üîç Categorical Features Analysis:\")\n",
    "\n",
    "for feature in categorical_features[:6]:  # Display first 6 features\n",
    "    print(f\"\\n{feature}:\")\n",
    "    value_counts = df[feature].value_counts()\n",
    "    print(value_counts)\n",
    "    \n",
    "    # Calculate churn rate for each category\n",
    "    churn_rate_by_category = df.groupby(feature)['Churn'].apply(lambda x: (x == 'Yes').mean() * 100)\n",
    "    print(f\"Churn rate by {feature}:\")\n",
    "    print(churn_rate_by_category.round(2))\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bba44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize categorical features vs churn\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(categorical_features[:9]):\n",
    "    # Create cross-tabulation\n",
    "    crosstab = pd.crosstab(df[feature], df['Churn'], normalize='index') * 100\n",
    "    \n",
    "    crosstab.plot(kind='bar', ax=axes[idx], color=['skyblue', 'salmon'])\n",
    "    axes[idx].set_title(f'Churn Rate by {feature}')\n",
    "    axes[idx].set_ylabel('Percentage')\n",
    "    axes[idx].legend(['No Churn', 'Churn'])\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Hide empty subplots\n",
    "for idx in range(len(categorical_features), len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c7245b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze numerical features\n",
    "print(\" Numerical Features Analysis:\")\n",
    "\n",
    "# Convert TotalCharges to numeric (handle any string values)\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "\n",
    "# Statistical summary for numerical features\n",
    "numerical_summary = df[numerical_features].describe()\n",
    "display(numerical_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046b3796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize numerical features distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(numerical_features[:4]):\n",
    "    # Handle missing values for visualization\n",
    "    feature_data = df[feature].dropna()\n",
    "    \n",
    "    # Distribution plot\n",
    "    sns.histplot(data=df.dropna(subset=[feature]), x=feature, hue='Churn', \n",
    "                kde=True, ax=axes[idx], alpha=0.7)\n",
    "    axes[idx].set_title(f'Distribution of {feature} by Churn')\n",
    "    axes[idx].legend(['No Churn', 'Churn'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6af6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for numerical features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(numerical_features[:4]):\n",
    "    sns.boxplot(data=df, x='Churn', y=feature, ax=axes[idx])\n",
    "    axes[idx].set_title(f'{feature} by Churn Status')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5565a5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "print(\"üîó Correlation Analysis:\")\n",
    "\n",
    "# Create a copy for correlation analysis\n",
    "df_corr = df.copy()\n",
    "\n",
    "# Remove customerID column if it exists (it's just an identifier, not useful for correlation)\n",
    "if 'customerID' in df_corr.columns:\n",
    "    df_corr = df_corr.drop('customerID', axis=1)\n",
    "\n",
    "# Encode categorical variables for correlation analysis\n",
    "label_encoders = {}\n",
    "for col in categorical_features + ['Churn']:\n",
    "    if col in df_corr.columns:  # Check if column exists\n",
    "        le = LabelEncoder()\n",
    "        df_corr[col] = le.fit_transform(df_corr[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "# Handle missing values\n",
    "df_corr = df_corr.fillna(0)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = df_corr.corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(16, 12))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .8}, fmt='.2f')\n",
    "plt.title('Correlation Matrix of All Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da06a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation with target (Churn)\n",
    "churn_correlation = correlation_matrix['Churn'].abs().sort_values(ascending=False)\n",
    "churn_correlation = churn_correlation.drop('Churn')  # Remove self-correlation\n",
    "\n",
    "print(\"Features Most Correlated with Churn:\")\n",
    "print(churn_correlation.head(15))\n",
    "\n",
    "# Visualize top correlations with churn\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_15_corr = churn_correlation.head(15)\n",
    "sns.barplot(x=top_15_corr.values, y=top_15_corr.index, palette='viridis')\n",
    "plt.title('Top 8 Features Correlated with Churn')\n",
    "plt.xlabel('Absolute Correlation with Churn')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407fe725",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing and Feature Engineering\n",
    "\n",
    "Handle missing values, encode categorical variables, scale numerical features, and create new features. Split data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af158bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our custom preprocessing module\n",
    "sys.path.append('src')\n",
    "from preprocessing import ChurnDataPreprocessor\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = ChurnDataPreprocessor()\n",
    "\n",
    "print(\"Preprocessor initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bf2ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data using preprocessor\n",
    "df_processed = preprocessor.load_data('data/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "print(f\"Original dataset shape: {df_processed.shape}\")\n",
    "\n",
    "# Basic data cleaning\n",
    "df_clean = preprocessor.basic_cleaning(df_processed)\n",
    "print(f\"After basic cleaning: {df_clean.shape}\")\n",
    "\n",
    "# Check for any changes\n",
    "print(\"\\n Changes made during cleaning:\")\n",
    "print(f\"- Columns removed: {set(df_processed.columns) - set(df_clean.columns)}\")\n",
    "print(f\"- TotalCharges converted to numeric\")\n",
    "print(f\"- Missing values in TotalCharges filled with 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1311d363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "df_engineered = preprocessor.feature_engineering(df_clean)\n",
    "print(f\" After feature engineering: {df_engineered.shape}\")\n",
    "\n",
    "# Display new features created\n",
    "new_features = set(df_engineered.columns) - set(df_clean.columns)\n",
    "print(f\"\\nNew features created: {new_features}\")\n",
    "\n",
    "# Show sample of new features\n",
    "if new_features:\n",
    "    print(\"\\n Sample of new features:\")\n",
    "    display(df_engineered[list(new_features)].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a09c662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify feature types\n",
    "numeric_features, categorical_features = preprocessor.identify_feature_types(df_engineered)\n",
    "\n",
    "# print(f\"\\n Numeric features ({len(numeric_features)}):\")\n",
    "# for i, feature in enumerate(numeric_features, 1):\n",
    "#     print(f\"{i:2d}. {feature}\")\n",
    "\n",
    "# print(f\"\\n Categorical features ({len(categorical_features)}):\")\n",
    "# for i, feature in enumerate(categorical_features, 1):\n",
    "#     print(f\"{i:2d}. {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71ba826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features\n",
    "df_encoded = preprocessor.encode_categorical_features(df_engineered, fit=True)\n",
    "print(f\" After encoding categorical features: {df_encoded.shape}\")\n",
    "\n",
    "# Show encoding examples\n",
    "print(\"\\n Encoding examples:\")\n",
    "for feature in categorical_features[:3]:\n",
    "    if feature in preprocessor.label_encoders:\n",
    "        encoder = preprocessor.label_encoders[feature]\n",
    "        print(f\"\\n{feature}: {dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dc5138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numeric features\n",
    "df_scaled = preprocessor.scale_numeric_features(df_encoded, fit=True)\n",
    "print(f\" After scaling numeric features: {df_scaled.shape}\")\n",
    "\n",
    "# Show scaling statistics\n",
    "print(\"\\n Scaling statistics (mean and std of scaled features):\")\n",
    "scaling_stats = pd.DataFrame({\n",
    "    'Feature': numeric_features,\n",
    "    'Mean_After_Scaling': df_scaled[numeric_features].mean().values,\n",
    "    'Std_After_Scaling': df_scaled[numeric_features].std().values\n",
    "})\n",
    "display(scaling_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e138f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X, y = preprocessor.prepare_features_target(df_scaled, target_col='Churn')\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Target distribution: {np.bincount(y)}\")\n",
    "print(f\"Target classes: {preprocessor.target_encoder.classes_}\")\n",
    "\n",
    "# Display feature names\n",
    "feature_names = X.columns.tolist()\n",
    "print(f\"\\n--Final feature list ({len(feature_names)} features):\")\n",
    "for i, feature in enumerate(feature_names, 1):\n",
    "    print(f\"{i:2d}. {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa255cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")\n",
    "print(f\"Training target distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Testing target distribution: {np.bincount(y_test)}\")\n",
    "\n",
    "# Calculate split percentages\n",
    "train_churn_rate = (y_train == 1).mean() * 100\n",
    "test_churn_rate = (y_test == 1).mean() * 100\n",
    "\n",
    "print(f\"\\n Training set churn rate: {train_churn_rate:.2f}%\")\n",
    "print(f\"Testing set churn rate: {test_churn_rate:.2f}%\")\n",
    "\n",
    "print(\"\\nData preprocessing completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2d94db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessor for later use\n",
    "os.makedirs('models', exist_ok=True)\n",
    "preprocessor.save_preprocessor('models/preprocessor.pkl')\n",
    "\n",
    "# Also save the processed data\n",
    "np.save('models/X_train.npy', X_train.values)\n",
    "np.save('models/X_test.npy', X_test.values)\n",
    "np.save('models/y_train.npy', y_train)\n",
    "np.save('models/y_test.npy', y_test)\n",
    "\n",
    "# Save feature names\n",
    "with open('models/feature_names.txt', 'w') as f:\n",
    "    for feature in feature_names:\n",
    "        f.write(f\"{feature}\\n\")\n",
    "\n",
    "print(\" Preprocessor and processed data saved successfully!\")\n",
    "print(\" Files saved:\")\n",
    "print(\"   - models/preprocessor.pkl\")\n",
    "print(\"   - models/X_train.npy\")\n",
    "print(\"   - models/X_test.npy\")\n",
    "print(\"   - models/y_train.npy\")\n",
    "print(\"   - models/y_test.npy\")\n",
    "print(\"   - models/feature_names.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2511d38f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we have:\n",
    "\n",
    "1. ‚úÖ **Loaded and explored** the Telco Customer Churn dataset\n",
    "2. ‚úÖ **Performed comprehensive EDA** with visualizations\n",
    "3. ‚úÖ **Analyzed feature relationships** and correlations with churn\n",
    "4. ‚úÖ **Implemented data preprocessing** pipeline including:\n",
    "   - Data cleaning and missing value handling\n",
    "   - Feature engineering (new features creation)\n",
    "   - Categorical encoding\n",
    "   - Numerical feature scaling\n",
    "   - Train-test split\n",
    "5. ‚úÖ **Saved preprocessor and data** for model training\n",
    "\n",
    "### Key Insights:\n",
    "- **Churn Rate**: ~27% of customers churn\n",
    "- **High Risk Factors**: Month-to-month contracts, Fiber optic internet, Electronic check payments\n",
    "- **Low Risk Factors**: Long-term contracts, Multiple services, Automatic payments\n",
    "- **Feature Engineering**: Created tenure groups, charge levels, service count, and average monthly charges\n",
    "\n",
    "### Next Steps:\n",
    "Move to the model training notebook (`02_Model_Training.ipynb`) to:\n",
    "- Train multiple machine learning models\n",
    "- Perform hyperparameter tuning\n",
    "- Evaluate and compare model performance\n",
    "- Select the best model for deployment"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
